\documentclass{sig-alternate}
\usepackage{times}
\usepackage{amssymb,amsmath}
\usepackage{cleveref}
\usepackage{algorithm}
\usepackage{times}
\usepackage{color}
\usepackage{url}
\usepackage{subfigure}
\usepackage{xspace}
\usepackage[noend]{algorithmic}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{balance} 
\usepackage{epstopdf}
\usepackage{graphicx}

\newcommand{\model}{{{\sf SourceNote}}\xspace}

\begin{document}

%\title{Assessing the Quality of Data and Finding Invaluable Sources}
%\title{Finding Quality in Volume: The Challenge of Discovering Invaluable Sources}
\title{SourceNote: Discovering Invaluable Data Sources in Volume}

\numberofauthors{5} 

\author{
% 1st. author
\alignauthor
Amol Deshpande\\
       \affaddr{University of Maryland}\\
       \email{amol@cs.umd.edu}
% 2nd. author
\alignauthor
Xin Luna Dong\\
       \affaddr{Google Inc.}\\
       \email{lunadong@google.com}
% 3rd. author
\alignauthor
Lise Getoor\\
       \affaddr{University of California, Santa Cruz}\\
       \email{getoor@soe.ucsc.edu}
 \and
% 4th. author
\alignauthor
Theodoros Rekatsinas\\
       \affaddr{University of Maryland}\\
       \email{thodrek@cs.umd.edu}
% 5th. author
\alignauthor
Divesh Srivastava\\
       \affaddr{AT\&T Labs Research}\\
       \email{divesh@research.att.com}
}    

\maketitle
\begin{abstract}
Data is becoming a commodity and integrating data from multiple data sources has tremendous value for many public and enterprise application domains. However, the number of data sources has risen rapidly due to recent developments in data publishing and availability over the web. The proliferation of services such as cloud-based data markets has facilitated the collection, publishing and trading of data. Furthermore, the adoption of open data policies both in science and government has increased the amount of open access data without restrictions or fees promoting the idea that data should be universally available. However, data sources are typically heterogeneous in their focus and content, they often providing duplicate and conflicting information and also vary significantly in terms of the accuracy and the timeliness of the data they provide. When the number of data sources is large, humans have a limited capability of extracting accurate estimates of source authoritativeness and quality.

In this paper we explore the problems of appraising, managing and reasoning about heterogeneous data sources for data integration and collective analysis. Given the sheer number of available data sources, analysts must (1) identify sources that potentially satisfy the needs of their applications with few effective clues about the content and quality of the sources, (2) repeatedly invest many man-hours in assessing the eventual usefulness of  these sources  by manually investigating their content or integrating subsets of them and evaluating the actual benefit of the integration result for their application. We propose \model, a vision for a data source management system that could dramatically ease the {\em Identify-Evaluate-Integrate} interaction loop that many analysts follows today to discover invaluable sources for their tasks.
\end{abstract}

% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%\terms{Theory}

%\keywords{ACM proceedings, \LaTeX, text tagging} % NOT required for Proceedings

\section{Introduction}

\ \\Describe how analyzing multiple data sources is part of modern applications (give examples from aggregators like datasift clients, OSI, political scientists). 

\ \\Talk about redundancy of sources, other characteristics that make it hard for the user to identify useful sources (have plots from GDELT). Show content diversity.

\ \\Conclude with paper structure: (1) describe challenges, including (i) how to formally define quality, (ii) how to reason about the estimate the quality of arbitrary sets of sources, (iii) how to support diverse tasks from different users and how the user will interact with the system, (2) present overview of \model and describe (i) support for heterogeneous sources diverse tasks, (ii) quality discovery module, dependency discovery and representation, (iii) query answering module. (3) related work conclusions.

\section{Challenges}
\subsection{Reasoning about the Quality of Sources}

\ \\Describe quality metrics and example of each quality metric: (i) coverage, (ii) freshness, (iii) timeliness, (iv) accuracy, (v) bias. 

\subsection{Estimating the Integration Quality}

\ \\Build quality profiles for individual sources and present how they can be combined using probabilities. 

\ \\Challenge of dependent sources. 

\subsection{Supporting Diverse Integration Tasks}

\ \\Present challenge of supporting diverse tasks. Give examples of diverse analytics tasks, structured vs. unstructured data. 

\subsection{User Interaction}
\ \\different cost functions (i) monetary, (ii) amount of data. How can a user identify and specify the right quality for her application. 

\ \\What are the right visualizations and interaction schemes.

\section{Framework Overview}

\ \\Present architecture framework. Core module {\em knowledge graph and correspondence graph}. Modules:
\begin{enumerate}
\item Query engine: free-text queries mapped to entities and concepts. Quality metrics specified, cost specified, time-points of interest. 
\item Correspondence graph: Go from entities to set of relevant sources (ground the domain of interest). 
\item Source quality estimation: Given quality annotations of correspondence graph estimate the quality for future time points.
\item Source selection module: Find source selection solutions on pareto frontier. Given quality and cost constraints find solutions on pareto frontier. 
\item Query optimizer: find diverse solutions on pareto frontier. Speculative query answering for neighborhood on pareto frontier. 
\item Result visualization: Present results to user. 
\end{enumerate}

\subsection{Supporting Diverse Integration Tasks}

\ \\Analyze concepts of knowledge graph and correspondence graph. Describe construction of correspondence graph. How can you compute quality. 

\subsection{Estimating the Quality of Integration}

\ \\Describe how a correspondence graph can be extended with factor graphs and describe compilation techniques to compute quality of integration. Learn dependencies for different metrics. The domain corresponds to unions of concepts and instances. 

\subsection{Query Processing}

\ \\Descibe finding solutions on pareto frontier, describe how to diversify solutions
\ \\How to visualize solutions (coverage, freshness source char plot from SIGMOD 2014), how to explore the pareto frontier and neighborhood of solutions; a two level approach: {\bf Level 1:} return ranked list with ``distant" solutions on the pareto curve just mention quality characteristics, number of sources and cost. Present an average characteristic vector, no names or individual characteristics of sources, {\bf Level 2:} once the user selects a solution from the ranked list, then present bubble graph based on characteristics of sources the two dimensions should be "concept focus and instance focus", bubble size should be source size (whatever the source is reporting). The user can click to remove a source from the solution and the characteristics (overall quality, cost) of the solution should be updated. Apart from removing sources neighboring solutions of the pareto curve should be presented in a separate list. 

\section{Related Work}

\section{Conclusions}

\bibliographystyle{abbrv}
\bibliography{srcmanage}
\end{document}
